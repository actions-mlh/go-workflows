My Farewell to Codegen - A Retrospective by Autumn Chiu (@jjayeon)

This project arose out of a recurring need at GitHub, which was this: knowing whether a GitHub Action workflow is valid before pushing it to a repo.  For most users, the only way to know whether or not a workflow will function as intended is to push that workflow into production, let GitHub's backend attempt to process it, and see what errors they receive.  This whole process can take several minutes, which is not too long for a preprocessing step; but if, for example, a workflow has a couple instances of tabs instead of spaces, or a colon missing from some lines, that can mean 3-5 minutes between fixing each syntax error, and that's before knowing if it functions semantically.  If we could develop a client-side parser that can read, understand, lint, and debug YAML workflows for the user, it would greatly reduce the friction in the experience.

Thus, Hank and I were assigned to build such a tool.  We were to build it in Go so that it could interoperate with GitHub's existing backend, and to start, we wanted to build something that could read a YAML file, scan it for errors, and print those errors to an output stream --- a simple linter.  Once we had the infrastructure to do that, it could be adapted for various other use cases around GitHub.

The first step was to read the YAML file itself.  Not too difficult, but since this was our first time working in Go, it gave us a chance to get familiar with its syntax.  Go has an official YAML-reading library (https://pkg.go.dev/gopkg.in/yaml.v3), which allowed us to skip over any business with syntax trees.  Our first version of the linter would simply read the YAML file using the package and report any errors that the package gives us.  In essence, this lets us check whether the file itself is valid YAML and catch errors like unexpected tokens, incorrect spacing, and misplaced colons; however, it says nothing about whether the YAML represents a valid GitHub workflow.

To do that, we would of course need a lot more code.  My first idea was to read the entire YAML file into an object of type `map[string]interface{}`, which is essentially Go's equivalent of a Javascript object.  (interface{} represents a type that can take on any value; in this context, either a primitive like a string, or another map, allowing for a nested structure.)  Since Go is a strongly typed language, making such liberal use of `interface{}` is generally discouraged, so we came up with two other possibilities.  One was to leverage the internal type representation of YAML that the YAML package itself uses; the other was to roll our own type system specific to GitHub Actions.  We ended up using a combination of both approaches: the current iteration will unpack the YAML file into a custom type hierarchy, but that hierarchy contains links to raw yaml.Node objects where appropriate, in case we want to analyze those instead.

Another idea discussed was using code generation to write our custom type system.  Since the GitHub workflow syntax is complicated, and there are already JSON schemas floating around that describe it, it seemed feasible to write a program that could convert a JSON schema into a series of Go types, customized according to our needs.  There's actually already a package that does this (https://github.com/a-h/generate), but its last update was three years ago and it was missing certain key features that our JSON schema uses, so I took on the task of hacking it to work for our purposes.  In the meantime, my partner Hank set out to build a prototype of the linter, writing a mocked-up version of what our generated code would look like as a stopgap.

It took a long time to pick through the package's internal details, but the simple version is this: the package will first unpack the JSON file using the canonical Go JSON package (https://pkg.go.dev/encoding/json), writing it into a Go custom type with all the fields of a schema: definitions, properties, oneOf, and so on.  Then it will scan through the various fields recursively, along the way tracking the structs and corresponding fields (of the structs) that it would need to output.  Finally, it would render all the data accumulated into a specified output stream, which can be a file or stdout or wherever else streams are used.  Going into the minutiae of the code would be overkill, but I can go over it if needed.

The package as written had a number of issues that made it untenable for our use case.  For one thing, certain fields of the JSON schema were more or less ignored; instances of oneOf, anyOf, or allOf were read from the JSON but quietly dropped, as with additionalProperties, and patternProperties was nowhere to be seen.  The JSON schema for GitHub actions makes extensive use of these features, so huge chunks of vital information were missing from the generated code.  Another issue was with the Items property: in the JSON schema, Items can either take the form of a JSON object or an array, but a-h/generate would only permit an object; an array would cause a syntax error, and cause the whole build to fail.  The code in general was also on the messy side, full of unclear functions, unused fields, missing error returns, and a general sense of redundancy and lack of focus.

I spent around 4-5 weeks hacking at a copy of the package; all the work I did still lives under the tag archive/codegen.  But in the end, we decided to scrap the codegen idea --- it was becoming far too much work to turn a JSON schema into sensible Go code, and our goal is to build a parser, not a code generator.  Also, the stopgap code that Hank had written turned out to be quite solid, and perfectly usable as a final product.  In my opinion, the big problem holding back the JSON/Go interoperation has to do with types.  In JSON and Javascript, variables and fields are weakly typed, meaning that you can write `a = 150` on one line and `a = "string"` on the next with no issue.  But in Go, values are strongly typed, meaning you can't switch between them easily; something defined as a string stays a string forever.  This is a problem with JSON schemas because JSON schemas make heavy use of weak typing.  For example, the additionalProperties field can, of course, be another JSON object with the additional properties needed; but an empty additionalProperties field is signified not by an empty object, but with the boolean `false`.  (It can also be omitted.)  In Javascript, this isn't a problem, but in Go, a value that can be either a Schema or a boolean is a nightmare to work with.

`a-h/generate` could work around it using some weird nested properties, but as the complexity of our needs grew, the amount of hacking needed for each workaround did too.  Another issue is with the oneOf field, which represents a JSON value that can take on one of several values, like a string, boolean, or even another schema.  As mentioned, this is not something that Go supports easily; the best we could do would be to turn a oneOf into an interface{} taking on several different values and validation code around it, but that's the issue we were trying to avoid in the first place.  As I got deeper into trying to fix every issue, it became clear that the problems weren't just matters of forgotten inputs and code cleanliness: there's a structural, systemic divide between JSON/Javascript and Go that cannot be crossed without compromising the integrity of Go's strong typing.

As I said, we decided to scrap the codegen --- it was a cool idea, but it's not the focus of this project, and as they say we have bigger fish to fry.  The work I put into codegen will live on in this archive, as a warning not to go down this path; or if you do, get familiar with our friend `interface{}`.  But in seriousness, I did learn a lot about Go, codegen, and JSON schemas out of all this, which I look forward to applying to a less doomed project.  As for you, my recommendation is to avoid using JSON and Go together, and if you need a complex type system, just write it yourself.
